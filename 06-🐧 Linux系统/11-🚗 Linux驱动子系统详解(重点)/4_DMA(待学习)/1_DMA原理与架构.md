# Linux DMA子系统学习指南 - 原理与架构

📢本篇将带领大家深刻理解`DMA`。

## 引言：为什么需要DMA

在学习DMA之前，让我们先思考一个问题：如果要从硬盘复制一个1GB的文件到内存，CPU需要做什么？

在没有DMA的情况下，CPU需要：

1. 从硬盘读取一个字节
2. 将这个字节写入内存
3. 重复上述步骤10亿次...

这期间CPU就像一个搬运工，不停地在硬盘和内存之间搬运数据，无法处理其他任务。这显然是对高性能CPU资源的极大浪费。

## 什么是DMA

> DMA (Direct Memory Access) is used to copy data directly between devices and RAM, without going through the CPU.

DMA是Direct Memory Access的缩写，顾名思义，就是绕开CPU直接访问memory的意思。在计算机中，相比CPU，memory和外设的速度是非常慢的，因而在memory和memory（或者memory和设备）之间搬运数据，非常浪费CPU的时间，造成CPU无法及时处理一些实时事件。因此，工程师们就设计出来一种专门用来搬运数据的器件----DMA控制器，协助CPU进行数据搬运。

![image.png|500](https://my-obsidian-image.oss-cn-guangzhou.aliyuncs.com/2025/06/eece2154142d30599b822e4af2f8c56a.png)

> [!tip]+ 理解要点 把DMA控制器想象成一个"数据搬运工"：CPU是老板，只需要告诉搬运工"把这堆货物从A仓库搬到B仓库"，然后就可以去处理其他重要事务了。搬运工（DMA）会自己完成搬运工作，完成后通知老板。

## DMA的产生：背景

过去几年里，整个计算机产业界，都在尝试不停地提升 `I/O` 设备的速度。把 `HDD` 硬盘换成 `SSD` 硬盘，我们仍然觉得不够快；用 `PCI Express` 接口的 `SSD` 硬盘替代 `SATA` 接口的 `SSD` 硬盘，我们还是觉得不够快。

所以，现在就有了傲腾（`Optane`）这样的技术。但是，无论 `I/O` 速度如何提升，比起 `CPU`，总还是太慢。`SSD` 硬盘的 `IOPS` 可以到 `2` 万、`4` 万，但是我们 `CPU` 的主频有 `2GHz` 以上，也就意味着每秒会有 `20` 亿次的操作。如果我们对于 `I/O` 的操作，都是由 `CPU` 发出对应的指令，然后等待 `I/O` 设备完成操作之后返回，那 `CPU` 有大量的时间其实都是在等待 `I/O` 设备完成操作。

但是，这个 `CPU` 的等待，在很多时候，其实并没有太多的实际意义。我们对于 `I/O` 设备的大量操作，其实都只是把内存里面的数据，传输到 `I/O` 设备而已。在这种情况下，其实 `CPU` 只是在傻等而已。特别是当传输的数据量比较大的时候，比如进行大文件复制，如果所有数据都要经过 `CPU`，实在是有点儿太浪费时间了。因此，计算机工程师们，就发明了 `DMA` 技术，也就是直接内存访问（`Direct Memory Access`）技术，来减少 `CPU` 等待的时间。

> [!note]+ 性能对比
> 
> - CPU主频：2GHz = 20亿次操作/秒
> - SSD IOPS：2-4万次/秒
> - 速度差异：约5万倍！
> 
> 这就像让一位每秒能处理20亿个任务的CEO，去做每秒只能完成4万次的搬运工作，效率极其低下。

## 理解 DMA：协处理器

其实 `DMA` 技术很容易理解，本质上，`DMA` 技术就是我们在主板上放一块独立的芯片。在进行内存和 `I/O` 设备的数据传输的时候，我们不再通过 `CPU` 来控制数据传输，而直接通过 `DMA` 控制器（`DMA Controller`，简称 `DMAC`）。这块芯片，我们可以认为它其实就是一个协处理器（Co-Processor）。

### DMA的价值体现

**`DMAC`** **最有价值的地方体现在，当我们要传输的数据特别大、速度特别快，或者传输的数据特别小、速度特别慢的时候。**

比如说，我们用千兆网卡或者硬盘传输大量数据的时候，如果都用 `CPU` 来搬运的话，肯定忙不过来，所以可以选择 `DMAC`。而当数据传输很慢的时候，`DMAC` 可以等数据到齐了，再发送信号，给到 `CPU` 去处理，而不是让 `CPU` 在那里忙等待。

### 为什么叫"协处理器"

好了，现在你应该明白 `DMAC` 的价值，知道了它适合用在什么情况下。那我们现在回过头来看。我们上面说，`DMAC` 是一块"协处理器芯片"，这是为什么呢？

注意，这里面的"协"字。`DMAC` 是在"协助"`CPU`，完成对应的数据传输工作。在 `DMAC` 控制数据传输的过程中，我们还是需要 `CPU` 的。

### 主设备与从设备

除此之外，`DMAC` 其实也是一个特殊的 `I/O` 设备，它和`CPU` 以及其他 `I/O` 设备一样，通过连接到总线来进行实际的数据传输。总线上的设备呢，其实有两种类型。一种我们称之为主设备（`Master`），另外一种，我们称之为从设备（`Slave`）。

想要主动发起数据传输，必须要是一个主设备才可以，`CPU` 就是主设备。而我们从设备（比如硬盘）只能接受数据传输。所以，如果通过 `CPU` 来传输数据，要么是 `CPU` 从 `I/O` 设备读数据，要么是 `CPU` 向 `I/O` 设备写数据。

这个时候你可能要问了，那我们的 `I/O` 设备不能向主设备发起请求么？可以是可以，不过这个发送的不是数据内容，而是控制信号。`I/O` 设备可以告诉 `CPU`，我这里有数据要传输给你，但是实际数据是 `CPU` 拉走的，而不是 `I/O` 设备推给 CPU 的。

![](https://ncn13z89mqjm.feishu.cn/space/api/box/stream/download/asynccode/?code=MTRkZjBhOWMzMzIxZmY2MzAxOGZlMzAzYWI1ZGFkZmVfVGp1MTR6aDRSSG42Q3h5OE44M1pyUldGZlNEb2U2NklfVG9rZW46VHZIUmJzYnVTb2syeTN4dUZKYmN0UE9ibmdoXzE3NTA5MjkxMzE6MTc1MDkzMjczMV9WNA)

不过，`DMAC` 就很有意思了，它既是一个主设备，又是一个从设备。对于 `CPU` 来说，它是一个从设备；对于硬盘这样的 `IO` 设备来说呢，它又变成了一个主设备。

> [!tip]+ 理解要点 DMAC的双重身份就像一个"中间管理者"：
> 
> - 对上级（CPU）：是个听命令的下属（从设备）
> - 对下级（I/O设备）：是个发号施令的领导（主设备）

### DMA传输过程详解

那使用 `DMAC` 进行数据传输的过程究竟是什么样的呢？下面我们来具体看看。

1. 首先，`CPU` 还是作为一个主设备，向 `DMAC` 设备发起请求。这个请求，其实就是在 `DMAC` 里面修改配置寄存器。
    
2. `CPU` 修改 `DMAC` 的配置的时候，会告诉 `DMAC` 这样几个信息：
    
    - 首先是源地址的初始值以及传输时候的地址增减方式。所谓源地址，就是数据要从哪里传输过来。如果我们要从内存里面写入数据到硬盘上，那么就是要读取的数据在内存里面的地址。如果是从硬盘读取数据到内存里，那就是硬盘的 `I/O` 接口的地址。我们讲过总线的时候说过，`I/O` 的地址可以是一个内存地址，也可以是一个端口地址。而地址的增减方式就是说，数据是从大的地址向小的地址传输，还是从小的地址往大的地址传输。
    - 其次是目标地址初始值和传输时候的地址增减方式。目标地址自然就是和源地址对应的设备，也就是我们数据传输的目的地。
    - 第三个自然是要传输的数据长度，也就是我们一共要传输多少数据。
3. 设置完这些信息之后，`DMAC` 就会变成一个空闲的状态（`Idle`）。
    
4. 如果我们要从硬盘上往内存里面加载数据，这个时候，硬盘就会向 `DMAC` 发起一个数据传输请求。这个请求并不是通过总线，而是通过一个额外的连线。
    
5. 然后，我们的 `DMAC` 需要再通过一个额外的连线响应这个申请。
    
6. 于是，`DMAC` 这个芯片，就向硬盘的接口发起要总线读的传输请求。数据就从硬盘里面，读到了 `DMAC` 的控制器里面。
    
7. 然后，`DMAC` 再向我们的内存发起总线写的数据传输请求，把数据写入到内存里面。
    
8. `DMAC` 会反复进行上面第 `6`、`7` 步的操作，直到 `DMAC` 的寄存器里面设置的数据长度传输完成。
    
9. 数据传输完成之后，`DMAC` 重新回到第 `3` 步的空闲状态。
    

所以，整个数据传输的过程中，我们不是通过 `CPU` 来搬运数据，而是由 `DMAC` 这个芯片来搬运数据。但是 `CPU` 在这个过程中也是必不可少的。因为传输什么数据，从哪里传输到哪里，其实还是由 `CPU` 来设置的。这也是为什么，`DMAC` 被叫作"协处理器"。

![image.png|500](https://my-obsidian-image.oss-cn-guangzhou.aliyuncs.com/2025/06/04f9712a4573f4e977fa3259c350406a.png)

> [!example]+ DMA传输流程示例 假设要从硬盘读取1MB数据到内存：
> 
> 1. CPU告诉DMAC："从硬盘地址0x1000读取1MB数据到内存地址0x2000"
> 2. CPU继续处理其他任务
> 3. DMAC自动完成数据搬运
> 4. DMAC通知CPU："搬运完成了！"
> 5. CPU处理搬运完成的数据

### DMA的发展历程

最早，计算机里是没有 `DMAC` 的，所有数据都是由 `CPU` 来搬运的。随着人们对于数据传输的需求越来越多，先是出现了主板上独立的 `DMAC` 控制器。到了今天，各种 `I/O` 设备越来越多，数据传输的需求越来越复杂，使用的场景各不相同。加之显示器、网卡、硬盘对于数据传输的需求都不一样，所以各个设备里面都有自己的 `DMAC` 芯片了。

## DMA硬件架构

了解了DMA的基本原理后，让我们深入了解DMA的硬件架构，这对理解Linux DMA子系统至关重要。

### DMA channels

一个DMA controller可以"同时"进行的DMA传输的个数是有限的，这称作DMA channels。当然，这里的channel，只是一个逻辑概念，因为：

> 鉴于总线访问的冲突，以及内存一致性的考量，从物理的角度看，不大可能会同时进行两个（及以上）的DMA传输。因而DMA channel不太可能是物理上独立的通道；
> 
> 很多时候，DMA channels是DMA controller为了方便，抽象出来的概念，让consumer以为独占了一个channel，实际上所有channel的DMA传输请求都会在DMA controller中进行仲裁，进而串行传输；
> 
> 因此，软件也可以基于controller提供的channel（我们称为"物理"channel），自行抽象更多的"逻辑"channel，软件会管理这些逻辑channel上的传输请求。实际上很多平台都这样做了，在DMA Engine framework中，不会区分这两种channel（本质上没区别）。

> [!tip]+ 理解要点 DMA channel就像高速公路的车道：虽然有多条车道，但通过收费站（总线）时还是要排队。channel给了每个设备"专属车道"的错觉，实际上大家还是要轮流使用总线。

### DMA request lines

DMA传输是由CPU发起的：CPU会告诉DMA控制器，帮忙将xxx地方的数据搬到xxx地方。CPU发完指令之后，就当甩手掌柜了。而DMA控制器，除了负责怎么搬之外，还要决定一件非常重要的事情（特别是有外部设备参与的数据传输）：

> 何时可以开始数据搬运？

因为，CPU发起DMA传输的时候，并不知道当前是否具备传输条件，例如source设备是否有数据、dest设备的FIFO是否空闲等等。那谁知道是否可以传输呢？设备！因此，需要DMA传输的设备和DMA控制器之间，会有几条物理的连接线（称作DMA request，DRQ），用于通知DMA控制器可以开始传输了。

这就是DMA request lines的由来，通常来说，每一个数据收发的节点（称作endpoint），和DMA controller之间，就有一条DMA request line（memory设备除外）。

最后总结：

> DMA channel是Provider（提供传输服务），DMA request line是Consumer（消费传输服务）。在一个系统中DMA request line的数量通常比DMA channel的数量多，因为并不是每个request line在每一时刻都需要传输。

> [!note]+ 类比理解
> 
> - DMA channel = 出租车（提供服务）
> - DMA request line = 乘客（需要服务）
> - 系统中乘客数量 > 出租车数量（不是每个乘客时刻都要打车）

### 传输参数

在最简单的DMA传输中，只需为DMA controller提供一个参数----transfer size，它就可以欢快的工作了：

> 在每一个时钟周期，DMA controller将1byte的数据从一个buffer搬到另一个buffer，直到搬完"transfer size"个bytes即可停止。

不过这在现实世界中往往不能满足需求，因为有些设备可能需要在一个时钟周期中，传输指定bit的数据，例如：

> memory之间传输数据的时候，希望能以总线的最大宽度为单位（32-bit、64-bit等），以提升数据传输的效率；
> 
> 而在音频设备中，需要每次写入精确的16-bit或者24-bit的数据；
> 
> 等等。

因此，为了满足这些多样的需求，我们需要为DMA controller提供一个额外的参数----transfer width。

另外，当传输的源或者目的地是memory的时候，为了提高效率，DMA controller不愿意每一次传输都访问memory，而是在内部开一个buffer，将数据缓存在自己buffer中：

> memory是源的时候，一次从memory读出一批数据，保存在自己的buffer中，然后再一点点（以时钟为节拍），传输到目的地；
> 
> memory是目的地的时候，先将源的数据传输到自己的buffer中，当累计一定量的数据之后，再一次性的写入memory。

这种场景下，DMA控制器内部可缓存的数据量的大小，称作burst size----另一个参数。

> [!tip]+ 关键参数总结 DMA传输的三个关键参数：
> 
> 1. **transfer size**：要搬运的数据总量（多少个单位）
> 2. **transfer width**：每次搬运的数据宽度（8/16/32/64 bit）
> 3. **burst size**：批量搬运的大小（减少内存访问次数）

### scatter-gather

一般情况下，DMA传输一般只能处理在物理上连续的buffer。但在有些场景下，我们需要将一些非连续的buffer拷贝到一个连续buffer中（这样的操作称作scatter gather，挺形象的）。

对于这种非连续的传输，大多时候都是通过软件，将传输分成多个连续的小块（chunk）。但为了提高传输效率（特别是在图像、视频等场景中），有些DMA controller从硬件上支持了这种操作。

注2：具体怎么支持，和硬件实现有关，这里不再多说（只需要知道有这个事情即可，编写DMA controller驱动的时候，自然会知道怎么做）。

> [!example]+ scatter-gather示例 假设内存中有3块不连续的数据：
> 
> - 块1：地址0x1000，大小1KB
> - 块2：地址0x3000，大小2KB
> - 块3：地址0x5000，大小1KB
> 
> scatter-gather DMA可以一次性将这3块数据收集起来，传输到目标地址的连续4KB空间中。

## 总结

通过本章的学习，我们了解了：

1. **DMA的本质**：一个专门的数据搬运协处理器
2. **DMA的价值**：解放CPU，提高系统效率
3. **DMA的工作原理**：CPU设置参数，DMA自主完成传输
4. **DMA的硬件架构**：
    - Channels：逻辑传输通道
    - Request lines：设备就绪信号
    - 传输参数：size、width、burst
    - Scatter-gather：非连续内存传输

> [!tip]+ 进阶提示 理解了DMA的硬件原理后，下一步就是学习Linux如何抽象和管理这些硬件资源，这就是DMA Engine子系统的作用。